{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM for Retrosynthesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up Environments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the scripts on the github page, set up the environment as follows: (I would adivse to use different pytorch and python versions as corrected below) <br>\n",
    "\n",
    "> ``` conda create -n mol_transformer python=3.6 ``` <br>\n",
    "> ``` conda activate mol_transformer ``` <br>\n",
    "> ``` conda install rdkit -c rdkit ``` <br>\n",
    "> ``` conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=9.2 -c pytorch ``` <br>\n",
    "> ``` conda install future six tqdm pandas ```<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the environment into the root of this notebook using: <br>\n",
    "> ``` git clone https://github.com/pschwllr/MolecularTransformer.git ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, run the following two lines of code to complete the setup (ensure to run *cd MolecularTransformer*):<br> \n",
    "> ``` pip install torchtext==0.3.1``` <br>\n",
    "> ```pip install -e . ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From raw data (this could be done in the morning sessions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # Must return true, otherwise training takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_smiles(smiles):\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smiles)]\n",
    "    assert smiles == ''.join(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles_from_df(df:pd.DataFrame) -> tuple:\n",
    "    reactions = df.iloc[:,-1].values\n",
    "    products, reactants = [], []\n",
    "    for reaction in reactions:\n",
    "        reac, pro = reaction.split('>>')\n",
    "        try:\n",
    "            pro_mol, reac_mol = Chem.MolFromSmiles(pro), Chem.MolFromSmiles(reac)\n",
    "            [a.SetAtomMapNum(0) for a in pro_mol.GetAtoms()]\n",
    "            [a.SetAtomMapNum(0) for a in reac_mol.GetAtoms()]\n",
    "            product_smi = Chem.MolToSmiles(pro_mol, canonical=True, isomericSmiles=True)\n",
    "            reactant_smi = Chem.MolToSmiles(reac_mol, canonical=True, isomericSmiles=True)\n",
    "            products.append(tokenize_smiles(product_smi)), reactants.append(tokenize_smiles(reactant_smi))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return products, reactants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to data\n",
    "raw_data_path = Path().cwd() / \"RawData\" \n",
    "uspto_path = Path().cwd() / 'MolecularTransformer' / 'data' / 'USPTO50'\n",
    "raw_test = pd.read_csv(raw_data_path / \"raw_test.csv\")\n",
    "raw_train = pd.read_csv(raw_data_path / \"raw_train.csv\")\n",
    "raw_val = pd.read_csv(raw_data_path / \"raw_val.csv\")\n",
    "\n",
    "# create input files for the LLM\n",
    "modes = ['train', 'val', 'test']\n",
    "for mode, df in zip(modes, [raw_train, raw_val, raw_test]):\n",
    "    df_new = pd.DataFrame()\n",
    "    products, reactants = get_smiles_from_df(df)\n",
    "    df_new['products'] = products\n",
    "    df_new['reactants'] = reactants\n",
    "    df_new[['products']].to_csv( uspto_path / f\"src-{mode}.txt\", index=False, header=False)\n",
    "    df_new[['reactants']].to_csv( uspto_path / f\"tgt-{mode}.txt\", index=False, header=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved the tokenized strings to source and target files in the USPTO50 folder. Now, we only have to perform one more preprocessing step before training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use bash (if on linux/mac) or cmd (if on windows) to run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd MolecularTransformer\n",
    "dataset=MIT_mixed_augm # MIT_mixed_augm / STEREO_mixed_augm\n",
    "python preprocess.py -train_src data/${dataset}/src-train.txt \\\n",
    "                     -train_tgt data/${dataset}/tgt-train.txt \\\n",
    "                     -valid_src data/${dataset}/src-val.txt \\\n",
    "                     -valid_tgt data/${dataset}/tgt-val.txt \\\n",
    "                     -save_data data/${dataset}/${dataset} \\\n",
    "                     -src_seq_length 1000 -tgt_seq_length 1000 \\\n",
    "                     -src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19044.2728]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks>set dataset=USPTO50\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks>cd MolecularTransformer\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>python preprocess.py -train_src data\\%dataset%\\src-train.txt ^\n",
      "More? -train_tgt data\\%dataset%\\tgt-train.txt ^\n",
      "More? -valid_src data\\%dataset%\\src-val.txt ^\n",
      "More? -valid_tgt data\\%dataset%\\tgt-val.txt ^\n",
      "More? -save_data data\\%dataset%%dataset% ^\n",
      "More? -src_seq_length 1000 -tgt_seq_length 1000 ^\n",
      "More? -src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-05 16:47:26,604 INFO] Extracting features...\n",
      "[2023-07-05 16:47:26,610 INFO]  * number of source features: 0.\n",
      "[2023-07-05 16:47:26,610 INFO]  * number of target features: 0.\n",
      "[2023-07-05 16:47:26,610 INFO] Building `Fields` object...\n",
      "[2023-07-05 16:47:26,610 INFO] Building & saving training data...\n",
      "[2023-07-05 16:47:26,610 INFO] Reading source and target files: data\\USPTO50\\src-train.txt data\\USPTO50\\tgt-train.txt.\n",
      "[2023-07-05 16:47:26,628 INFO] Splitting shard 0.\n",
      "[2023-07-05 16:47:26,642 INFO] Building shard 0.\n",
      "[2023-07-05 16:47:30,451 INFO]  * saving 0th train data shard to data\\USPTO50USPTO50.train.0.pt.\n",
      "[2023-07-05 16:47:31,602 INFO] Building & saving validation data...\n",
      "[2023-07-05 16:47:31,608 INFO] Reading source and target files: data\\USPTO50\\src-val.txt data\\USPTO50\\tgt-val.txt.\n",
      "[2023-07-05 16:47:31,610 INFO] Splitting shard 0.\n",
      "[2023-07-05 16:47:31,613 INFO] Building shard 0.\n",
      "[2023-07-05 16:47:32,084 INFO]  * saving 0th valid data shard to data\\USPTO50USPTO50.valid.0.pt.\n",
      "[2023-07-05 16:47:32,262 INFO] Building & saving vocabulary...\n",
      "[2023-07-05 16:47:32,913 INFO]  * reloading data\\USPTO50USPTO50.train.0.pt.\n",
      "[2023-07-05 16:47:33,248 INFO]  * tgt vocab size: 69.\n",
      "[2023-07-05 16:47:33,248 INFO]  * src vocab size: 59.\n",
      "[2023-07-05 16:47:33,248 INFO]  * merging src and tgt vocab...\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "set dataset=USPTO50\n",
    "cd MolecularTransformer\n",
    "python preprocess.py -train_src data\\%dataset%\\src-train.txt ^\n",
    "-train_tgt data\\%dataset%\\tgt-train.txt ^\n",
    "-valid_src data\\%dataset%\\src-val.txt ^\n",
    "-valid_tgt data\\%dataset%\\tgt-val.txt ^\n",
    "-save_data data\\%dataset%%dataset% ^\n",
    "-src_seq_length 1000 -tgt_seq_length 1000 ^\n",
    "-src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training takes around 3 hours on the hpc cluster, however, checkpoints are written constantly. I have saved the last 20 checkpoints in the MolecularTransformer\\checkpoints\\USPTO50 folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the training either use one of the following two commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dataset=USPTO50\n",
    "cd MolecularTransformer\n",
    "\n",
    "python  train.py -data data/${dataset}/${dataset} \\\n",
    "                   -save_model experiments/checkpoints/${dataset}/${dataset}_model \\\n",
    "                   -seed 42 -gpu_ranks 0 -save_checkpoint_steps 10000 -keep_checkpoint 20 \\\n",
    "                   -train_steps 500000 -param_init 0  -param_init_glorot -max_generator_batches 32 \\\n",
    "                   -batch_size 4096 -batch_type tokens -normalization tokens -max_grad_norm 0  -accum_count 4 \\\n",
    "                   -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam -warmup_steps 8000  \\\n",
    "                   -learning_rate 2 -label_smoothing 0.0 -report_every 1000 \\\n",
    "                   -layers 4 -rnn_size 256 -word_vec_size 256 -encoder_type transformer -decoder_type transformer \\\n",
    "                   -dropout 0.1 -position_encoding -share_embeddings \\\n",
    "                   -global_attention general -global_attention_function softmax -self_attn_type scaled-dot \\\n",
    "                   -heads 8 -transformer_ff 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19044.2728]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks>set dataset=USPTO50\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks>cd MolecularTransformer\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>python train.py -data data%dataset%%dataset% ^\n",
      "More? -save_model experiments\\checkpoints%dataset%%dataset%_model ^\n",
      "More? -seed 42 -gpu_ranks 0 -save_checkpoint_steps 10000 -keep_checkpoint 20 ^\n",
      "More? -train_steps 500000 -param_init 0 -param_init_glorot -max_generator_batches 32 ^\n",
      "More? -batch_size 4096 -batch_type tokens -normalization tokens -max_grad_norm 0 -accum_count 4 ^\n",
      "More? -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 ^\n",
      "More? -learning_rate 2 -label_smoothing 0.0 -report_every 1000 ^\n",
      "More? -layers 4 -rnn_size 256 -word_vec_size 256 -encoder_type transformer -decoder_type transformer ^\n",
      "More? -dropout 0.1 -position_encoding -share_embeddings ^\n",
      "More? -global_attention general -global_attention_function softmax -self_attn_type scaled-dot ^\n",
      "More? -heads 8 -transformer_ff 2048\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 118, in <module>\n",
      "    main(opt)\n",
      "  File \"train.py\", line 51, in main\n",
      "    single_main(opt, 0)\n",
      "  File \"c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer\\onmt\\train_single.py\", line 86, in main\n",
      "    opt = training_opt_postprocessing(opt, device_id)\n",
      "  File \"c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer\\onmt\\train_single.py\", line 77, in training_opt_postprocessing\n",
      "    torch.cuda.set_device(device_id)\n",
      "  File \"c:\\Users\\fgh18\\Anaconda3\\envs\\mol_transformer\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 263, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "AttributeError: module 'torch._C' has no attribute '_cuda_setDevice'\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "set dataset=USPTO50\n",
    "cd MolecularTransformer\n",
    "\n",
    "python train.py -data data%dataset%%dataset% ^\n",
    "-save_model experiments\\checkpoints%dataset%%dataset%_model ^\n",
    "-seed 42 -gpu_ranks 0 -save_checkpoint_steps 10000 -keep_checkpoint 20 ^\n",
    "-train_steps 500000 -param_init 0 -param_init_glorot -max_generator_batches 32 ^\n",
    "-batch_size 4096 -batch_type tokens -normalization tokens -max_grad_norm 0 -accum_count 4 ^\n",
    "-optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 ^\n",
    "-learning_rate 2 -label_smoothing 0.0 -report_every 1000 ^\n",
    "-layers 4 -rnn_size 256 -word_vec_size 256 -encoder_type transformer -decoder_type transformer ^\n",
    "-dropout 0.1 -position_encoding -share_embeddings ^\n",
    "-global_attention general -global_attention_function softmax -self_attn_type scaled-dot ^\n",
    "-heads 8 -transformer_ff 2048"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns an error since I do not have the cuda package installed on this local machine. Otherwise, it would start training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, we simply need to put our tokenized test molecule in the MolecularTransformer\\data\\USPTO50 folder.\n",
    "We could either load a single model from the checkpoint (below) or build an ensemble of models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure to create a results folder in the experiments directory first"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Linux/OS, you simply have to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd MolecularTransformer\n",
    "dataset=USPTO50\n",
    "model=${dataset}_model_step_500000.pt\n",
    "\n",
    "python translate.py -model experiments/checkpoints/${dataset}/${model} \\\n",
    "                    -src data/${dataset}/src-test.txt \\\n",
    "                    -output experiments/results/predictions_${model}_on_${dataset}_test.txt \\\n",
    "                    -batch_size 64 -replace_unk -max_length 200 -fast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Windows, you run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19044.2728]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks>cd MolecularTransformer\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>set dataset=USPTO50\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>set model=%dataset%_model_step_500000.pt\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>python translate.py -model experiments\\checkpoints\\%dataset%\\%model% ^\n",
      "More?                     -src data\\%dataset%\\src-test.txt ^\n",
      "More?                     -output experiments\\results\\predictions_%model%_on_%dataset%_test.txt ^\n",
      "More?                     -batch_size 64 -replace_unk -max_length 200 -fast\n",
      "\n",
      "(mol_transformer) c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fgh18\\Anaconda3\\envs\\mol_transformer\\lib\\site-packages\\torchtext\\data\\field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "Traceback (most recent call last):\n",
      "  File \"translate.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"translate.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer\\onmt\\translate\\translator.py\", line 235, in translate\n",
      "    batch_data = self.translate_batch(batch, data, fast=self.fast)\n",
      "  File \"c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer\\onmt\\translate\\translator.py\", line 342, in translate_batch\n",
      "    return_attention=self.replace_unk)\n",
      "  File \"c:\\Users\\fgh18\\Documents\\Chem_LLM_Hackathon\\example_notebooks\\MolecularTransformer\\onmt\\translate\\translator.py\", line 471, in _fast_translate_batch\n",
      "    [alive_seq.index_select(0, select_indices),\n",
      "RuntimeError: index_select(): Expected dtype int64 for index\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "cd MolecularTransformer\n",
    "set dataset=USPTO50\n",
    "set model=%dataset%_model_step_500000.pt\n",
    "\n",
    "python translate.py -model experiments\\checkpoints\\%dataset%\\%model% ^\n",
    "                    -src data\\%dataset%\\src-test.txt ^\n",
    "                    -output experiments\\results\\predictions_%model%_on_%dataset%_test.txt ^\n",
    "                    -batch_size 64 -replace_unk -max_length 200 -fast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mol_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
